{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "23f9bb45",
   "metadata": {},
   "source": [
    "# üé≠ Facial Emotion Recognition - Model Training\n",
    "\n",
    "This notebook provides a comprehensive pipeline for training a facial emotion recognition model using the FER2013 dataset.\n",
    "\n",
    "## Contents:\n",
    "1. **Setup & Dataset Download** - Install packages and download FER2013\n",
    "2. **Data Loading & Exploration** - Explore dataset structure\n",
    "3. **Data Visualization** - Class distribution and sample images\n",
    "4. **Data Preprocessing** - Augmentation and generators\n",
    "5. **Model Architecture** - Define CNN model\n",
    "6. **Training** - Train the model with callbacks\n",
    "7. **Evaluation** - Analyze model performance\n",
    "8. **Export** - Save model in multiple formats\n",
    "\n",
    "---\n",
    "**Dataset**: FER2013 (48x48 grayscale images, 7 emotion classes)\n",
    "- Emotions: Angry, Disgust, Fear, Happy, Sad, Surprise, Neutral\n",
    "- Training samples: ~28,709\n",
    "- Test samples: ~3,589"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c497092d",
   "metadata": {},
   "source": [
    "## 1. Setup and Dataset Download\n",
    "Install required packages and download the FER2013 dataset from Kaggle using kagglehub."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6af1bfcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages (run once)\n",
    "# !pip install kagglehub tensorflow opencv-python matplotlib seaborn scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4239373",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# TensorFlow and Keras\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.layers import (\n",
    "    Conv2D, MaxPooling2D, Dense, Dropout, Flatten,\n",
    "    BatchNormalization, GlobalAveragePooling2D, Input\n",
    ")\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import (\n",
    "    ModelCheckpoint, EarlyStopping, \n",
    "    ReduceLROnPlateau, TensorBoard\n",
    ")\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "# Sklearn for evaluation\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "# Download dataset\n",
    "import kagglehub\n",
    "\n",
    "print(f\"TensorFlow version: {tf.__version__}\")\n",
    "print(f\"GPU available: {tf.config.list_physical_devices('GPU')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a717356",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download FER2013 dataset using kagglehub\n",
    "dataset_path = kagglehub.dataset_download(\"msambare/fer2013\")\n",
    "print(f\"Dataset downloaded to: {dataset_path}\")\n",
    "\n",
    "# Define paths\n",
    "TRAIN_DIR = os.path.join(dataset_path, 'train')\n",
    "TEST_DIR = os.path.join(dataset_path, 'test')\n",
    "\n",
    "# Create models directory for saving\n",
    "MODELS_DIR = Path('../models')\n",
    "MODELS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(f\"\\nTrain directory: {TRAIN_DIR}\")\n",
    "print(f\"Test directory: {TEST_DIR}\")\n",
    "print(f\"Models directory: {MODELS_DIR.absolute()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9c24a29",
   "metadata": {},
   "source": [
    "## 2. Data Loading and Exploration\n",
    "Explore the dataset structure and count images per emotion category."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4a9b5b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define emotion classes\n",
    "EMOTIONS = ['angry', 'disgust', 'fear', 'happy', 'sad', 'surprise', 'neutral']\n",
    "NUM_CLASSES = len(EMOTIONS)\n",
    "IMG_SIZE = (48, 48)\n",
    "\n",
    "# Count images per category\n",
    "def count_images(directory):\n",
    "    \"\"\"Count images in each emotion subdirectory\"\"\"\n",
    "    counts = {}\n",
    "    for emotion in EMOTIONS:\n",
    "        emotion_dir = os.path.join(directory, emotion)\n",
    "        if os.path.exists(emotion_dir):\n",
    "            counts[emotion] = len([f for f in os.listdir(emotion_dir) \n",
    "                                   if f.endswith(('.jpg', '.png', '.jpeg'))])\n",
    "        else:\n",
    "            counts[emotion] = 0\n",
    "    return counts\n",
    "\n",
    "# Get counts\n",
    "train_counts = count_images(TRAIN_DIR)\n",
    "test_counts = count_images(TEST_DIR)\n",
    "\n",
    "# Create summary DataFrame\n",
    "summary_df = pd.DataFrame({\n",
    "    'Emotion': EMOTIONS,\n",
    "    'Train': [train_counts[e] for e in EMOTIONS],\n",
    "    'Test': [test_counts[e] for e in EMOTIONS]\n",
    "})\n",
    "summary_df['Total'] = summary_df['Train'] + summary_df['Test']\n",
    "summary_df['Train %'] = (summary_df['Train'] / summary_df['Train'].sum() * 100).round(2)\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"üìä DATASET SUMMARY\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"\\nüèãÔ∏è Training samples: {summary_df['Train'].sum():,}\")\n",
    "print(f\"üß™ Test samples: {summary_df['Test'].sum():,}\")\n",
    "print(f\"üìÅ Total samples: {summary_df['Total'].sum():,}\")\n",
    "print(f\"üéØ Number of classes: {NUM_CLASSES}\")\n",
    "print(f\"üìê Image size: {IMG_SIZE}\")\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(summary_df.to_string(index=False))\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3efb050d",
   "metadata": {},
   "source": [
    "## 3. Data Visualization - Class Distribution\n",
    "Visualize the distribution of emotion classes in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59986b69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set style for professional visualizations\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "# Color palette for emotions\n",
    "emotion_colors = {\n",
    "    'angry': '#FF6B6B',\n",
    "    'disgust': '#4ECDC4',\n",
    "    'fear': '#9B59B6',\n",
    "    'happy': '#F39C12',\n",
    "    'sad': '#3498DB',\n",
    "    'surprise': '#E74C3C',\n",
    "    'neutral': '#95A5A6'\n",
    "}\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "# Bar chart - Training distribution\n",
    "colors = [emotion_colors[e] for e in EMOTIONS]\n",
    "bars = axes[0].bar(EMOTIONS, [train_counts[e] for e in EMOTIONS], color=colors, edgecolor='white', linewidth=1.5)\n",
    "axes[0].set_title('Training Set Distribution', fontsize=14, fontweight='bold')\n",
    "axes[0].set_xlabel('Emotion', fontsize=12)\n",
    "axes[0].set_ylabel('Number of Images', fontsize=12)\n",
    "axes[0].tick_params(axis='x', rotation=45)\n",
    "for bar, count in zip(bars, [train_counts[e] for e in EMOTIONS]):\n",
    "    axes[0].text(bar.get_x() + bar.get_width()/2, bar.get_height() + 100, \n",
    "                 f'{count:,}', ha='center', va='bottom', fontsize=9, fontweight='bold')\n",
    "\n",
    "# Pie chart - Training distribution\n",
    "axes[1].pie([train_counts[e] for e in EMOTIONS], labels=EMOTIONS, autopct='%1.1f%%',\n",
    "            colors=colors, explode=[0.02]*7, shadow=True, startangle=90)\n",
    "axes[1].set_title('Training Set Proportion', fontsize=14, fontweight='bold')\n",
    "\n",
    "# Grouped bar chart - Train vs Test\n",
    "x = np.arange(len(EMOTIONS))\n",
    "width = 0.35\n",
    "bars1 = axes[2].bar(x - width/2, [train_counts[e] for e in EMOTIONS], width, label='Train', color='steelblue', alpha=0.8)\n",
    "bars2 = axes[2].bar(x + width/2, [test_counts[e] for e in EMOTIONS], width, label='Test', color='coral', alpha=0.8)\n",
    "axes[2].set_title('Train vs Test Distribution', fontsize=14, fontweight='bold')\n",
    "axes[2].set_xlabel('Emotion', fontsize=12)\n",
    "axes[2].set_ylabel('Number of Images', fontsize=12)\n",
    "axes[2].set_xticks(x)\n",
    "axes[2].set_xticklabels(EMOTIONS, rotation=45)\n",
    "axes[2].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('../models/class_distribution.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nüìà Class distribution chart saved to models/class_distribution.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9531b0a",
   "metadata": {},
   "source": [
    "## 4. Data Visualization - Sample Images\n",
    "Display sample images from each emotion category to understand the visual characteristics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa54ca43",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "from PIL import Image\n",
    "\n",
    "def load_sample_images(directory, emotion, n_samples=5):\n",
    "    \"\"\"Load sample images from an emotion directory\"\"\"\n",
    "    emotion_dir = os.path.join(directory, emotion)\n",
    "    images = []\n",
    "    if os.path.exists(emotion_dir):\n",
    "        files = [f for f in os.listdir(emotion_dir) if f.endswith(('.jpg', '.png', '.jpeg'))][:n_samples]\n",
    "        for f in files:\n",
    "            img_path = os.path.join(emotion_dir, f)\n",
    "            img = cv2.imread(img_path, cv2.IMREAD_GRAYSCALE)\n",
    "            if img is not None:\n",
    "                images.append(img)\n",
    "    return images\n",
    "\n",
    "# Create grid of sample images\n",
    "fig, axes = plt.subplots(7, 6, figsize=(14, 16))\n",
    "fig.suptitle('Sample Images per Emotion Class', fontsize=16, fontweight='bold', y=1.02)\n",
    "\n",
    "for i, emotion in enumerate(EMOTIONS):\n",
    "    samples = load_sample_images(TRAIN_DIR, emotion, n_samples=6)\n",
    "    \n",
    "    # Add emotion label on the left\n",
    "    axes[i, 0].set_ylabel(emotion.upper(), fontsize=12, fontweight='bold', \n",
    "                          rotation=0, ha='right', va='center',\n",
    "                          color=emotion_colors[emotion])\n",
    "    \n",
    "    for j, img in enumerate(samples):\n",
    "        if j < 6:\n",
    "            axes[i, j].imshow(img, cmap='gray')\n",
    "            axes[i, j].axis('off')\n",
    "    \n",
    "    # Fill empty slots\n",
    "    for j in range(len(samples), 6):\n",
    "        axes[i, j].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('../models/sample_images.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nüñºÔ∏è Sample images grid saved to models/sample_images.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f494bd2",
   "metadata": {},
   "source": [
    "## 5. Data Preprocessing and Augmentation\n",
    "Create data generators with augmentation for training and simple rescaling for validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7c4e3d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "BATCH_SIZE = 64\n",
    "TARGET_SIZE = IMG_SIZE  # (48, 48)\n",
    "\n",
    "# Training data generator with augmentation\n",
    "train_datagen = ImageDataGenerator(\n",
    "    rescale=1./255,\n",
    "    rotation_range=15,\n",
    "    width_shift_range=0.15,\n",
    "    height_shift_range=0.15,\n",
    "    shear_range=0.15,\n",
    "    zoom_range=0.15,\n",
    "    horizontal_flip=True,\n",
    "    fill_mode='nearest'\n",
    ")\n",
    "\n",
    "# Validation/Test data generator - only rescaling\n",
    "test_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "# Create generators\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    TRAIN_DIR,\n",
    "    target_size=TARGET_SIZE,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    color_mode='grayscale',\n",
    "    class_mode='categorical',\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "validation_generator = test_datagen.flow_from_directory(\n",
    "    TEST_DIR,\n",
    "    target_size=TARGET_SIZE,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    color_mode='grayscale',\n",
    "    class_mode='categorical',\n",
    "    shuffle=False\n",
    ")\n",
    "\n",
    "print(\"\\n‚úÖ Data generators created successfully!\")\n",
    "print(f\"üì¶ Training batches: {len(train_generator)}\")\n",
    "print(f\"üì¶ Validation batches: {len(validation_generator)}\")\n",
    "print(f\"üè∑Ô∏è Class indices: {train_generator.class_indices}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e3716a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize augmented images\n",
    "fig, axes = plt.subplots(2, 5, figsize=(15, 6))\n",
    "fig.suptitle('Data Augmentation Examples', fontsize=14, fontweight='bold')\n",
    "\n",
    "# Get a batch of images\n",
    "sample_batch = next(train_generator)\n",
    "images, labels = sample_batch\n",
    "\n",
    "for i in range(10):\n",
    "    row, col = i // 5, i % 5\n",
    "    axes[row, col].imshow(images[i].squeeze(), cmap='gray')\n",
    "    emotion_idx = np.argmax(labels[i])\n",
    "    axes[row, col].set_title(EMOTIONS[emotion_idx], fontsize=10)\n",
    "    axes[row, col].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Reset generator\n",
    "train_generator.reset()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "657f7ef5",
   "metadata": {},
   "source": [
    "## 6. Model Architecture Definition\n",
    "Define a CNN model with multiple convolutional blocks, batch normalization, and dropout for regularization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef13639c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_emotion_model(input_shape=(48, 48, 1), num_classes=7):\n",
    "    \"\"\"\n",
    "    Build a CNN model for facial emotion recognition.\n",
    "    \n",
    "    Architecture:\n",
    "    - 4 Convolutional blocks with increasing filters\n",
    "    - Batch normalization after each conv layer\n",
    "    - MaxPooling and Dropout for regularization\n",
    "    - Dense layers with dropout\n",
    "    \"\"\"\n",
    "    model = Sequential([\n",
    "        # Block 1\n",
    "        Conv2D(64, (3, 3), padding='same', activation='relu', input_shape=input_shape),\n",
    "        BatchNormalization(),\n",
    "        Conv2D(64, (3, 3), padding='same', activation='relu'),\n",
    "        BatchNormalization(),\n",
    "        MaxPooling2D(pool_size=(2, 2)),\n",
    "        Dropout(0.25),\n",
    "        \n",
    "        # Block 2\n",
    "        Conv2D(128, (3, 3), padding='same', activation='relu'),\n",
    "        BatchNormalization(),\n",
    "        Conv2D(128, (3, 3), padding='same', activation='relu'),\n",
    "        BatchNormalization(),\n",
    "        MaxPooling2D(pool_size=(2, 2)),\n",
    "        Dropout(0.25),\n",
    "        \n",
    "        # Block 3\n",
    "        Conv2D(256, (3, 3), padding='same', activation='relu'),\n",
    "        BatchNormalization(),\n",
    "        Conv2D(256, (3, 3), padding='same', activation='relu'),\n",
    "        BatchNormalization(),\n",
    "        MaxPooling2D(pool_size=(2, 2)),\n",
    "        Dropout(0.25),\n",
    "        \n",
    "        # Block 4\n",
    "        Conv2D(512, (3, 3), padding='same', activation='relu'),\n",
    "        BatchNormalization(),\n",
    "        Conv2D(512, (3, 3), padding='same', activation='relu'),\n",
    "        BatchNormalization(),\n",
    "        MaxPooling2D(pool_size=(2, 2)),\n",
    "        Dropout(0.25),\n",
    "        \n",
    "        # Fully Connected Layers\n",
    "        Flatten(),\n",
    "        Dense(512, activation='relu'),\n",
    "        BatchNormalization(),\n",
    "        Dropout(0.5),\n",
    "        Dense(256, activation='relu'),\n",
    "        BatchNormalization(),\n",
    "        Dropout(0.5),\n",
    "        Dense(num_classes, activation='softmax')\n",
    "    ])\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Build model\n",
    "model = build_emotion_model(input_shape=(48, 48, 1), num_classes=NUM_CLASSES)\n",
    "\n",
    "# Display model summary\n",
    "model.summary()\n",
    "\n",
    "# Visualize model architecture\n",
    "print(\"\\nüìê Model Architecture Summary:\")\n",
    "print(f\"   Total layers: {len(model.layers)}\")\n",
    "print(f\"   Total params: {model.count_params():,}\")\n",
    "print(f\"   Trainable params: {sum([np.prod(w.shape) for w in model.trainable_weights]):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e57e22d6",
   "metadata": {},
   "source": [
    "## 7. Model Compilation and Callbacks\n",
    "Compile the model and set up training callbacks for monitoring and optimization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b41544a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the model\n",
    "LEARNING_RATE = 0.0001\n",
    "\n",
    "model.compile(\n",
    "    optimizer=Adam(learning_rate=LEARNING_RATE),\n",
    "    loss='categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "# Calculate class weights for imbalanced dataset\n",
    "train_labels = train_generator.classes\n",
    "class_weights = compute_class_weight(\n",
    "    class_weight='balanced',\n",
    "    classes=np.unique(train_labels),\n",
    "    y=train_labels\n",
    ")\n",
    "class_weight_dict = dict(enumerate(class_weights))\n",
    "\n",
    "print(\"üìä Class weights for imbalanced data:\")\n",
    "for idx, emotion in enumerate(EMOTIONS):\n",
    "    print(f\"   {emotion}: {class_weight_dict[idx]:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b20b264",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define callbacks\n",
    "EPOCHS = 50\n",
    "\n",
    "callbacks = [\n",
    "    # Save best model\n",
    "    ModelCheckpoint(\n",
    "        filepath=str(MODELS_DIR / 'emotion_model_best.keras'),\n",
    "        monitor='val_accuracy',\n",
    "        save_best_only=True,\n",
    "        mode='max',\n",
    "        verbose=1\n",
    "    ),\n",
    "    \n",
    "    # Early stopping\n",
    "    EarlyStopping(\n",
    "        monitor='val_loss',\n",
    "        patience=10,\n",
    "        restore_best_weights=True,\n",
    "        verbose=1\n",
    "    ),\n",
    "    \n",
    "    # Reduce learning rate on plateau\n",
    "    ReduceLROnPlateau(\n",
    "        monitor='val_loss',\n",
    "        factor=0.5,\n",
    "        patience=5,\n",
    "        min_lr=1e-7,\n",
    "        verbose=1\n",
    "    ),\n",
    "    \n",
    "    # TensorBoard logging\n",
    "    TensorBoard(\n",
    "        log_dir=str(MODELS_DIR / 'logs'),\n",
    "        histogram_freq=1\n",
    "    )\n",
    "]\n",
    "\n",
    "print(\"‚úÖ Callbacks configured:\")\n",
    "print(\"   - ModelCheckpoint: Save best model\")\n",
    "print(\"   - EarlyStopping: Stop if no improvement for 10 epochs\")\n",
    "print(\"   - ReduceLROnPlateau: Reduce LR by 0.5 if no improvement for 5 epochs\")\n",
    "print(\"   - TensorBoard: Training logs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e894852b",
   "metadata": {},
   "source": [
    "## 8. Model Training\n",
    "Train the model with the configured callbacks and class weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22405ee4",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üöÄ Starting training...\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(\n",
    "    train_generator,\n",
    "    steps_per_epoch=len(train_generator),\n",
    "    epochs=EPOCHS,\n",
    "    validation_data=validation_generator,\n",
    "    validation_steps=len(validation_generator),\n",
    "    callbacks=callbacks,\n",
    "    class_weight=class_weight_dict,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"‚úÖ Training completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb72c6bd",
   "metadata": {},
   "source": [
    "## 9. Training History Visualization\n",
    "Plot training and validation accuracy/loss curves to analyze model learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e7a39e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training history\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Accuracy plot\n",
    "axes[0].plot(history.history['accuracy'], label='Training Accuracy', linewidth=2, color='steelblue')\n",
    "axes[0].plot(history.history['val_accuracy'], label='Validation Accuracy', linewidth=2, color='coral')\n",
    "axes[0].set_title('Model Accuracy Over Epochs', fontsize=14, fontweight='bold')\n",
    "axes[0].set_xlabel('Epoch', fontsize=12)\n",
    "axes[0].set_ylabel('Accuracy', fontsize=12)\n",
    "axes[0].legend(loc='lower right', fontsize=10)\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "axes[0].set_ylim([0, 1])\n",
    "\n",
    "# Loss plot\n",
    "axes[1].plot(history.history['loss'], label='Training Loss', linewidth=2, color='steelblue')\n",
    "axes[1].plot(history.history['val_loss'], label='Validation Loss', linewidth=2, color='coral')\n",
    "axes[1].set_title('Model Loss Over Epochs', fontsize=14, fontweight='bold')\n",
    "axes[1].set_xlabel('Epoch', fontsize=12)\n",
    "axes[1].set_ylabel('Loss', fontsize=12)\n",
    "axes[1].legend(loc='upper right', fontsize=10)\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('../models/training_history.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# Print best results\n",
    "best_val_acc = max(history.history['val_accuracy'])\n",
    "best_epoch = history.history['val_accuracy'].index(best_val_acc) + 1\n",
    "print(f\"\\nüìà Best validation accuracy: {best_val_acc:.4f} at epoch {best_epoch}\")\n",
    "print(f\"üìâ Final training accuracy: {history.history['accuracy'][-1]:.4f}\")\n",
    "print(f\"üìâ Final validation accuracy: {history.history['val_accuracy'][-1]:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "280d0040",
   "metadata": {},
   "source": [
    "## 10. Model Evaluation and Confusion Matrix\n",
    "Evaluate the model on test data and generate detailed performance metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50fee2fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate on test set\n",
    "print(\"üß™ Evaluating model on test set...\")\n",
    "validation_generator.reset()\n",
    "test_loss, test_accuracy = model.evaluate(validation_generator, verbose=1)\n",
    "\n",
    "print(f\"\\nüìä Test Results:\")\n",
    "print(f\"   Loss: {test_loss:.4f}\")\n",
    "print(f\"   Accuracy: {test_accuracy:.4f} ({test_accuracy*100:.2f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05a4cf53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate predictions\n",
    "validation_generator.reset()\n",
    "predictions = model.predict(validation_generator, verbose=1)\n",
    "predicted_classes = np.argmax(predictions, axis=1)\n",
    "true_classes = validation_generator.classes\n",
    "\n",
    "# Classification report\n",
    "print(\"\\nüìã Classification Report:\")\n",
    "print(\"=\" * 60)\n",
    "print(classification_report(true_classes, predicted_classes, target_names=EMOTIONS))\n",
    "\n",
    "# Confusion matrix\n",
    "cm = confusion_matrix(true_classes, predicted_classes)\n",
    "\n",
    "# Plot confusion matrix\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# Raw counts\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "            xticklabels=EMOTIONS, yticklabels=EMOTIONS, ax=axes[0])\n",
    "axes[0].set_title('Confusion Matrix (Counts)', fontsize=14, fontweight='bold')\n",
    "axes[0].set_xlabel('Predicted', fontsize=12)\n",
    "axes[0].set_ylabel('Actual', fontsize=12)\n",
    "\n",
    "# Normalized\n",
    "cm_normalized = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "sns.heatmap(cm_normalized, annot=True, fmt='.2%', cmap='Blues',\n",
    "            xticklabels=EMOTIONS, yticklabels=EMOTIONS, ax=axes[1])\n",
    "axes[1].set_title('Confusion Matrix (Normalized)', fontsize=14, fontweight='bold')\n",
    "axes[1].set_xlabel('Predicted', fontsize=12)\n",
    "axes[1].set_ylabel('Actual', fontsize=12)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('../models/confusion_matrix.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nüìä Confusion matrix saved to models/confusion_matrix.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa86885a",
   "metadata": {},
   "source": [
    "## 11. Model Export and Saving\n",
    "Save the trained model in multiple formats for deployment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a57b074",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model in different formats\n",
    "print(\"üíæ Saving model in multiple formats...\")\n",
    "\n",
    "# 1. Keras native format (recommended)\n",
    "keras_path = MODELS_DIR / 'emotion_model.keras'\n",
    "model.save(keras_path)\n",
    "print(f\"   ‚úÖ Keras format: {keras_path}\")\n",
    "\n",
    "# 2. H5 format (legacy, for compatibility)\n",
    "h5_path = MODELS_DIR / 'emotion_model.h5'\n",
    "model.save(h5_path)\n",
    "print(f\"   ‚úÖ H5 format: {h5_path}\")\n",
    "\n",
    "# 3. SavedModel format (for TensorFlow Serving)\n",
    "savedmodel_path = MODELS_DIR / 'emotion_model_savedmodel'\n",
    "model.save(savedmodel_path)\n",
    "print(f\"   ‚úÖ SavedModel format: {savedmodel_path}\")\n",
    "\n",
    "# 4. Save model architecture to JSON\n",
    "architecture_path = MODELS_DIR / 'model_architecture.json'\n",
    "with open(architecture_path, 'w') as f:\n",
    "    f.write(model.to_json())\n",
    "print(f\"   ‚úÖ Architecture JSON: {architecture_path}\")\n",
    "\n",
    "# 5. Save training history\n",
    "history_df = pd.DataFrame(history.history)\n",
    "history_path = MODELS_DIR / 'training_history.csv'\n",
    "history_df.to_csv(history_path, index=False)\n",
    "print(f\"   ‚úÖ Training history: {history_path}\")\n",
    "\n",
    "# 6. Save model summary to text file\n",
    "summary_path = MODELS_DIR / 'model_summary.txt'\n",
    "with open(summary_path, 'w') as f:\n",
    "    model.summary(print_fn=lambda x: f.write(x + '\\n'))\n",
    "    f.write(f\"\\n{'='*60}\\n\")\n",
    "    f.write(f\"Final Test Accuracy: {test_accuracy:.4f}\\n\")\n",
    "    f.write(f\"Final Test Loss: {test_loss:.4f}\\n\")\n",
    "    f.write(f\"Best Validation Accuracy: {best_val_acc:.4f} at epoch {best_epoch}\\n\")\n",
    "print(f\"   ‚úÖ Model summary: {summary_path}\")\n",
    "\n",
    "print(\"\\nüéâ All models and artifacts saved successfully!\")\n",
    "print(f\"üìÅ Models directory: {MODELS_DIR.absolute()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ce23119",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final summary\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"üèÜ TRAINING COMPLETE - SUMMARY\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"\"\"\n",
    "üìä Dataset:\n",
    "   - Training samples: {train_generator.samples:,}\n",
    "   - Test samples: {validation_generator.samples:,}\n",
    "   - Classes: {NUM_CLASSES} emotions\n",
    "\n",
    "üß† Model:\n",
    "   - Architecture: Custom CNN (4 conv blocks)\n",
    "   - Total parameters: {model.count_params():,}\n",
    "   - Input shape: {IMG_SIZE} grayscale\n",
    "\n",
    "üìà Performance:\n",
    "   - Best validation accuracy: {best_val_acc:.4f} ({best_val_acc*100:.2f}%)\n",
    "   - Test accuracy: {test_accuracy:.4f} ({test_accuracy*100:.2f}%)\n",
    "   - Test loss: {test_loss:.4f}\n",
    "\n",
    "üíæ Saved Files:\n",
    "   - emotion_model.keras (recommended)\n",
    "   - emotion_model.h5 (legacy)\n",
    "   - emotion_model_savedmodel/ (TF Serving)\n",
    "   - model_architecture.json\n",
    "   - training_history.csv\n",
    "   - confusion_matrix.png\n",
    "   - training_history.png\n",
    "\n",
    "üöÄ Next Steps:\n",
    "   1. Run main.py to start real-time emotion detection\n",
    "   2. The model will be loaded from models/emotion_model.keras\n",
    "\"\"\")\n",
    "print(\"=\" * 60)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
